@book{bandura1971,
  title = {Social Learning Theory},
  author = {Bandura, Albert},
  year = {1971},
  publisher = {General Learning Press},
  address = {New York},
  file = {/Users/blira/Zotero/storage/9GCBT9WG/Bandura_SocialLearningTheory-2.pdf}
}

@misc{bastani2024,
  title = {Generative {{AI Can Harm Learning}}},
  author = {Bastani, Hamsa and Bastani, Osbert and Sungu, Alp and Ge, Haosen and Kabakc{\i}, {\"O}zge and Mariman, Rei},
  year = {2024},
  doi = {10.2139/ssrn.4895486},
  urldate = {2024-07-23},
  langid = {english},
  file = {/Users/blira/Zotero/storage/I89LRJ83/Bastani et al. - 2024 - Generative AI Can Harm Learning.pdf}
}

@article{brynjolfsson2023,
  title = {Generative {{AI}} at Work},
  author = {Brynjolfsson, Erik and Li, Danielle and Raymond, Lindsay R.},
  year = {2023},
  journal = {NBER Working Paper Series},
  volume = {Working Paper 31161},
  file = {/Users/blira/Zotero/storage/ATZQGTWA/w31161.pdf}
}

@misc{bubeck2023,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = apr,
  number = {arXiv:2303.12712},
  eprint = {2303.12712},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-23},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4 [Ope23], was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/blira/Zotero/storage/SYYDEVD6/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf}
}

@article{costello,
  title = {Title: {{Durably}} Reducing Conspiracy Beliefs through Dialogues with {{AI}}},
  author = {Costello, Thomas H and Pennycook, Gordon and Rand, David G},
  abstract = {Conspiracy theories are a paradigmatic example of beliefs that, once adopted, are extremely difficult to dispel. Influential psychological theories propose that conspiracy beliefs 10 are uniquely resistant to counterevidence because they satisfy important needs and motivations.},
  langid = {english},
  file = {/Users/blira/Zotero/storage/EPF7DF2D/Costello et al. - Title Durably reducing conspiracy beliefs through.pdf}
}

@article{dellacqua2023,
  title = {Navigating the {{Jagged Technological Frontier}}: {{Field Experimental Evidence}} of the {{Effects}} of {{AI}} on {{Knowledge Worker Productivity}} and {{Quality}}},
  shorttitle = {Navigating the {{Jagged Technological Frontier}}},
  author = {Dell'Acqua, Fabrizio and McFowland, Edward and Mollick, Ethan R. and {Lifshitz-Assaf}, Hila and Kellogg, Katherine and Rajendran, Saran and Krayer, Lisa and Candelon, Fran{\c c}ois and Lakhani, Karim R.},
  year = {2023},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4573321},
  urldate = {2024-01-22},
  langid = {english},
  file = {/Users/blira/Zotero/storage/P2JZLTC2/Dell'Acqua et al. - 2023 - Navigating the Jagged Technological Frontier Fiel.pdf}
}

@article{extance2023,
  title = {{{ChatGPT}} Has Entered the Classroom: How {{LLMs}} Could Transform Education},
  author = {Extance, Andy},
  year = {2023},
  journal = {Nature},
  volume = {623},
  pages = {474--477},
  file = {/Users/blira/Zotero/storage/R2PP2HSR/d41586-023-03507-3.pdf}
}

@article{fisher2015,
  title = {Searching for Explanations: {{How}} the {{Internet}} Inflates Estimates of Internal Knowledge.},
  shorttitle = {Searching for Explanations},
  author = {Fisher, Matthew and Goddu, Mariel K. and Keil, Frank C.},
  year = {2015},
  month = jun,
  journal = {Journal of Experimental Psychology: General},
  volume = {144},
  number = {3},
  pages = {674--687},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/xge0000070},
  urldate = {2024-07-23},
  abstract = {As the Internet has become a nearly ubiquitous resource for acquiring knowledge about the world, questions have arisen about its potential effects on cognition. Here we show that searching the Internet for explanatory knowledge creates an illusion whereby people mistake access to information for their own personal understanding of the information. Evidence from 9 experiments shows that searching for information online leads to an increase in self-assessed knowledge as people mistakenly think they have more knowledge ``in the head,'' even seeing their own brains as more active as depicted by functional MRI (fMRI) images.},
  langid = {english},
  file = {/Users/blira/Zotero/storage/7T4G6QQS/Fisher et al. - 2015 - Searching for explanations How the Internet infla.pdf}
}

@techreport{harding2024,
  title = {Coding on {{Copilot}}: 2023 {{Data Shows Downward Pressure}} on {{Code Quality}}},
  author = {Harding, William and Kloster, Matthew},
  year = {2024},
  pages = {1--24},
  file = {/Users/blira/Zotero/storage/MT8UXPCJ/Coding-on-Copilot-2024-Developer-Research.pdf}
}

@article{jurenka2024,
  title = {Towards {{Responsible Development}} of {{Generative AI}} for {{Education}}: {{An Evaluation-Driven Approach}}},
  author = {Jurenka, Irina and Kunesch, Markus and McKee, Kevin and Gillick, Daniel and Zhu, Shaojian and Wiltberger, Sara and Phal, Shubham Milind and Hermann, Katherine and Kasenberg, Daniel and Bhoopchand, Avishkar and Anand, Ankit and P{\^i}slar, Miruna and Chan, Stephanie and Wang, Lisa and She, Jennifer and Mahmoudieh, Parsa and Rysbek, Aliya and Huber, Andrea and Wiltshire, Brett and Elidan, Gal and Rabin, Roni and Rubinovitz, Jasmin and Pitaru, Amit and Wilkowski, Julia and Choi, David and Engelberg, Roee and Hackmon, Lidan and Levin, Adva and Griffin, Rachel and Sears, Michael and Bar, Filip and Mesar, Mia and Jabbour, Mana and Chaudhry, Arslan and Cohan, James and Levine, Nir and Brown, Ben and Gorur, Dilan and Grant, Svetlana and Hashimoshoni, Rachel and Hu, Jieru and Chen, Dawn and Dolecki, Kuba and Akbulut, Canfer and Bileschi, Maxwell and Culp, Laura and Dong, Wen-Xin and Marchal, Nahema and Deman, Kelsie Van and Bajaj, Hema and Duah, Michael and Ambar, Moran and Lefdal, Sandra and Summerfield, Chris and An, James and Kamienny, Pierre-Alexandre and Mohdi, Abhinit and Strinopoulous, Theofilos and Hale, Annie and Anderson, Wayne and Cobo, Luis C and Efron, Niv and Ananda, Muktha and Mohamed, Shakir and Heymans, Maureen and Ghahramani, Zoubin and Matias, Yossi and Gomes, Ben and Ibrahim, Lila},
  year = {2024},
  langid = {english},
  file = {/Users/blira/Zotero/storage/LN7MABLA/Jurenka et al. - Towards Responsible Development of Generative AI f.pdf}
}

@article{kumar2023,
  title = {Math {{Education}} with {{Large Language Models}}: {{Peril}} or {{Promise}}?},
  shorttitle = {Math {{Education}} with {{Large Language Models}}},
  author = {Kumar, Harsh and Rothschild, David M. and Goldstein, Daniel G. and Hofman, Jake},
  year = {2023},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4641653},
  urldate = {2024-07-23},
  abstract = {The widespread availability of large language models (LLMs) has provoked both fear and excitement in the domain of education. On one hand, there is the concern that students will offload their coursework to LLMs, limiting what they themselves learn. On the other hand, there is the hope that LLMs might serve as scalable, personalized tutors. Here we conduct a large, pre-registered experiment involving 1200 participants to investigate how exposure to LLM-based explanations affect learning. In the experiment's learning phase, we gave participants practice problems and manipulated two key factors in a between-participants design: first, whether they were required to attempt a problem before or after seeing the correct answer, and second, whether participants were shown only the answer or were also exposed to an LLM-generated explanation of the answer. Subsequently, all participants were tested on new test questions to assess how well they had learned the underlying concepts. Overall we found that LLM-based explanations positively impacted learning relative to seeing only correct answers. The benefits were largest for those who attempted problems on their own first before consulting LLM explanations, but surprisingly this trend held even for those participants who were exposed to LLM explanations before attempting to solve practice problems on their own. An accompanying qualitative analysis revealed that these boosts in performance were indeed due to participants adopting the strategies they were shown, and that exposure to LLM explanations increased the amount people felt they learned and decreased the perceived difficulty of the test problems.},
  langid = {english},
  file = {/Users/blira/Zotero/storage/K9I4INMB/Kumar et al. - 2023 - Math Education with Large Language Models Peril o.pdf}
}

@article{maguire2000,
  title = {Navigation-Related Structural Change in the Hippocampi of Taxi Drivers},
  author = {Maguire, Eleanor A. and Gadian, David G. and Johnsrude, Ingrid S. and Good, Catriona D. and Ashburner, John and Frackowiak, Richard S. J. and Frith, Christopher D.},
  year = {2000},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {97},
  number = {8},
  pages = {4398--4403},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.070039597},
  urldate = {2024-01-30},
  abstract = {Structural MRIs of the brains of humans with extensive navigation experience, licensed London taxi drivers, were analyzed and compared with those of control subjects who did not drive taxis. The posterior hippocampi of taxi drivers were significantly larger relative to those of control subjects. A more anterior hippocampal region was larger in control subjects than in taxi drivers. Hippocampal volume correlated with the amount of time spent as a taxi driver (positively in the posterior and negatively in the anterior hippocampus). These data are in accordance with the idea that the posterior hippocampus stores a spatial representation of the environment and can expand regionally to accommodate elaboration of this representation in people with a high dependence on navigational skills. It seems that there is a capacity for local plastic change in the structure of the healthy adult human brain in response to environmental demands.},
  langid = {english}
}

@inproceedings{markel2023,
  title = {{{GPTeach}}: {{Interactive TA Training}} with {{GPT-based Students}}},
  shorttitle = {{{GPTeach}}},
  booktitle = {Proceedings of the {{Tenth ACM Conference}} on {{Learning}} @ {{Scale}}},
  author = {Markel, Julia M. and Opferman, Steven G. and Landay, James A. and Piech, Chris},
  year = {2023},
  month = jul,
  pages = {226--236},
  publisher = {ACM},
  address = {Copenhagen Denmark},
  doi = {10.1145/3573051.3593393},
  urldate = {2024-07-23},
  abstract = {Interactive and realistic teacher training is hard to scale. This is a key issue for learning at scale, as inadequate preparation can negatively impact both students and teachers. What if we could make the teacher training experience more engaging and, as a downstream effect, reduce the potential for harm that teachers-in-training could inflict on students? We present GPTeach, an interactive chat-based teacher training tool that allows novice teachers to practice with simulated students. We performed two studies to evaluate GPTeach: one think-aloud study and one A/B test between our tool and a baseline. Participants took the role of a teaching assistant conducting office hours with two GPT-simulated students. We found that our tool provides the opportunity for teachers to get valuable teaching practice without the pressures of affecting real students, allowing them to iterate their responses both during and across sessions. Additionally, participants enjoyed flexibility in tailoring their responses according to the varied personas, needs, and learning goals. In this paper, we provide quantitative results and qualitative observations to inform future work in this area. We conclude with a discussion of actionable design ideas for such systems, as well as other ways to use this tool for evaluating teachers and students. GPTeach has recently been deployed into the teacher training component of an online course with over 800 novice teachers.},
  isbn = {9798400700255},
  langid = {english},
  file = {/Users/blira/Zotero/storage/N5B92RC6/Markel et al. - 2023 - GPTeach Interactive TA Training with GPT-based St.pdf}
}

@article{mueller2014,
  title = {The Pen Is Mightier than the Keyboard: {{Advantages}} of Longhand over Laptop Note Taking},
  author = {Mueller, Pam A. and Oppenheimer, Daniel M.},
  year = {2014},
  month = apr,
  journal = {Psychological Science},
  doi = {10.1177/0956797614524581},
  abstract = {Taking notes on laptops rather than in longhand is increasingly common. Many researchers have suggested that laptop note taking is less effective than longhand note taking for learning. Prior studies have primarily focused on students' capacity for multitasking and distraction when using laptops. The present research suggests that even when laptops are used solely to take notes, they may still be impairing learning because their use results in shallower processing. In three studies, we found that students who took notes on laptops performed worse on conceptual questions than students who took notes longhand. We show that whereas taking more notes can be beneficial, laptop note takers' tendency to transcribe lectures verbatim rather than processing information and reframing it in their own words is detrimental to learning.},
  file = {/Users/blira/Zotero/storage/4ZXBAA9A/ACFrOgCEAxxj_bMdHjhj3q2XSnhwf2YD66i88c6khWeKSo.pdf}
}

@article{noy2023,
  title = {Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence},
  author = {Noy, Shakked and Zhang, Whitney},
  year = {2023},
  journal = {Science},
  volume = {381},
  pages = {187--192},
  langid = {english},
  file = {/Users/blira/Zotero/storage/RS5462KG/Noy and Zhang - 2023 - Experimental evidence on the productivity effects .pdf}
}

@misc{openai2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mo and Belgum, Jeff and Bello, Irwan and Berdine, Jake and {Bernadett-Shapiro}, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Sim{\'o}n Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and {Gontijo-Lopes}, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, {\L}ukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, {\L}ukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and M{\'e}ly, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cer{\'o}n and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
  year = {2023},
  month = dec,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-23},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformerbased model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/blira/Zotero/storage/XDW66KWV/OpenAI et al. - 2023 - GPT-4 Technical Report.pdf}
}

@book{rogers2023,
  title = {Writing for {{Busy Readers}}: {{Communicate More Effectively}} in the {{Real World}}},
  shorttitle = {Writing for {{Busy Readers}}},
  author = {Rogers, Todd and {Lasky-Fink}, Jessica},
  year = {2023},
  month = sep,
  publisher = {Penguin},
  abstract = {Writing well is for school. Writing effectively is for life.   Todd Rogers and Jessica Lasky-Fink offer the most valuable practical writing advice today. Building on their own research in behavioral science, they outline cognitive facts about how people actually read and distill them into six principles that will transform the power of your writing: Less is moreMake reading easyDesign for easy navigationUse enough formatting, but no moreTell readers why they should careMake responding easyIncluding many real-world examples, a checklist and other tools, this guide will make you a more successful and productive communicator. Rogers and Lasky-Fink bring Strunk and White's core ideas into the twenty-first century's attention marketplace.  ~ When~the influential guides to writing prose were written, the internet hadn't been invented. Now, the average American adult is inundated with digital messages each day. With all this correspondence, capturing a busy reader's attention is more challenging than ever. This is how to do it.},
  googlebooks = {bvCfEAAAQBAJ},
  isbn = {978-0-593-18748-7},
  langid = {english},
  keywords = {Business & Economics / Business Writing,Language Arts & Disciplines / Style Manuals,Language Arts & Disciplines / Writing / General}
}

@article{shulman2024,
  title = {Reading Dies in Complexity: {{Online}} News Consumers Prefer Simple Writing},
  shorttitle = {Reading Dies in Complexity},
  author = {Shulman, Hillary C. and Markowitz, David M. and Rogers, Todd},
  year = {2024},
  month = jun,
  journal = {Science Advances},
  volume = {10},
  number = {23},
  pages = {eadn2555},
  issn = {2375-2548},
  doi = {10.1126/sciadv.adn2555},
  urldate = {2024-07-23},
  abstract = {Over 30,000 field experiments with               The Washington Post               and               Upworthy               showed that readers prefer simpler headlines (e.g., more common words and more readable writing) over more complex ones. A follow-up mechanism experiment showed that readers from the general public paid more attention to, and processed more deeply, the simpler headlines compared to the complex headlines. That is, a signal detection study suggested readers were guided by a simpler-writing heuristic, such that they skipped over relatively complex headlines to focus their attention on the simpler headlines. Notably, a sample of professional writers, including journalists, did not show this pattern, suggesting that those writing the news may read it differently from those consuming it. Simplifying writing can help news outlets compete in the competitive online attention economy, and simple language can make news more approachable to online readers.                        ,              News readers engage more with simple writing, suggesting that journalists should write simply to attract attention online.},
  langid = {english},
  file = {/Users/blira/Zotero/storage/LKQS5ZX9/Shulman et al. - 2024 - Reading dies in complexity Online news consumers .pdf}
}

@article{singer2023a,
  title = {In {{Classrooms}}, {{Teachers Put A}}.{{I}}. {{Tutoring Bots}} to the {{Test}}},
  author = {Singer, Natasha},
  year = {2023},
  month = jun,
  journal = {The New York Times},
  issn = {0362-4331},
  urldate = {2024-07-23},
  abstract = {Newark public schools are cautiously trying out a new automated teaching aid from Khan Academy. The preliminary report card: ``could use improvement.''},
  chapter = {Technology},
  langid = {american},
  keywords = {Artificial Intelligence,ChatGPT,Education (K-12),Khan Academy,Khanmigo,Newark (NJ),OpenAI Labs,tutoring bots},
  file = {/Users/blira/Zotero/storage/7WNIY7PK/newark-schools-khan-tutoring-ai.html}
}

@article{sparrow2011,
  title = {Google {{Effects}} on {{Memory}}: {{Cognitive Consequences}} of {{Having Information}} at {{Our Fingertips}}},
  shorttitle = {Google {{Effects}} on {{Memory}}},
  author = {Sparrow, Betsy and Liu, Jenny and Wegner, Daniel M.},
  year = {2011},
  month = aug,
  journal = {Science},
  volume = {333},
  number = {6043},
  pages = {776--778},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1207745},
  urldate = {2024-01-27},
  abstract = {Owing to Internet search, we are more likely to encode ``where'' aspects of memory rather than ``what.''           ,              The advent of the Internet, with sophisticated algorithmic search engines, has made accessing information as easy as lifting a finger. No longer do we have to make costly efforts to find the things we want. We can ``Google'' the old classmate, find articles online, or look up the actor who was on the tip of our tongue. The results of four studies suggest that when faced with difficult questions, people are primed to think about computers and that when people expect to have future access to information, they have lower rates of recall of the information itself and enhanced recall instead for where to access it. The Internet has become a primary form of external or transactive memory, where information is stored collectively outside ourselves.},
  langid = {english},
  file = {/Users/blira/Zotero/storage/K3BJA8JD/Sparrow et al. - 2011 - Google Effects on Memory Cognitive Consequences o.pdf}
}
